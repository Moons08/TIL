{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "- corpus\n",
    "- tokenizing\n",
    "- morpheme\n",
    "- POS tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus ; 말뭉치\n",
    "자연어 분석 작업을 위해 만든 샘플 문서 집합 ( 기초 트레이닝용 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had been supplied\n",
      "by an excellent woman as governess, who had fallen little short\n",
      "of a mother in affection.\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.  Between _them_ it was more the intimacy\n",
      "of sisters.  Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness of her temper had hardly allowed\n",
      "her to impose any restraint; and the shadow of authority being\n",
      "now long passed away, they had been living together as friend and\n",
      "friend very mutually attached, and Emma doing just what she liked;\n",
      "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
      "her own.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "print(emma_raw[:1302])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing ; 토큰 생성\n",
    "\n",
    "긴 문자열을 분석을 위한 작은 단위로 나누는 일 (= tokenizing). 영문의 경우 문장, 단어 등을 토큰으로 사용하거나 정규 표현식을 쓸 수 있다.\n",
    "- 나뉘어진 문자열 단위 = token\n",
    "- 문자열을 토큰으로 분리하는 함수 = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixteen years had Miss Taylor been in Mr. Woodhouse's family,\n",
      "less as a governess than a friend, very fond of both daughters,\n",
      "but particularly of Emma.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "# sentence 문장/ 마침표를 이용하여 찾아냄\n",
    "print(sent_tokenize(emma_raw[:1000])[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize # 단어로 자름\n",
    "word_tokenize(emma_raw[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "t = RegexpTokenizer(\"[\\w]+\") # regex, customize\n",
    "t.tokenize(emma_raw[50:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# morpheme ; 형태소\n",
    "언어학에서 일정한 의미가 있는 가장 작은 말의 단위. 대부분 NLP에서는 토큰으로 형태소를 이용한다. *morphological analysis에서 파생*\n",
    "- stemming (어간 추출)\n",
    "- lemmatizing (원형 복원)\n",
    "- Part-Of-Speech tagging (품사 부착)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['flies', 'dies', 'denied', 'plotted', 'meeting', \n",
    "         'itemization', 'sensational', 'traditional']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming \n",
    "줄기만 남기고 앞뒤로 자름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fli', 'die', 'deni', 'plot', 'meet', 'item', 'sensat', 'tradit']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "[st.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fli', 'die', 'deny', 'plot', 'meet', 'item', 'sens', 'tradit']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "[st.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing\n",
    "같은 의미를 가지는 여러 단어를 가장 근본적인 '원형', 사전형으로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly',\n",
       " 'dy',\n",
       " 'denied',\n",
       " 'plotted',\n",
       " 'meeting',\n",
       " 'itemization',\n",
       " 'sensational',\n",
       " 'traditional']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "[lm.lemmatize(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('denied', 'deny')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize(\"denied\", pos=\"n\"), lm.lemmatize(\"denied\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging ; 품사 \n",
    "\n",
    "낱말을 문법적 기능, 형태, 뜻에 따라 구분한것을 태깅\n",
    "\n",
    "- NN 명사\n",
    "- PRP 인칭대명사\n",
    "- CD 서수\n",
    "- DT 관형사\n",
    "- VBP 동사 현재형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('volume', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('chapter', 'VBP'),\n",
       " ('1', 'CD'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('boy', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x = [\"volume\", \"I\", \"chapter\", \"1\", \"I\", \"am\", \"a\", \"boy\", \".\"]\n",
    "tagged_list = pos_tag(x)\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volume', 'I', 'chapter', '1', 'I', 'am', 'a', 'boy', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag(tagged_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-Learn에서 분석할때 tuple은 인식을 못하기 때문에 한개의 문자열로 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volume/NN',\n",
       " 'I/PRP',\n",
       " 'chapter/VBP',\n",
       " '1/CD',\n",
       " 'I/PRP',\n",
       " 'am/VBP',\n",
       " 'a/DT',\n",
       " 'boy/NN',\n",
       " './.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(doc):\n",
    "    return [\"/\".join(p) for p in pos_tag(doc)]\n",
    "tokenizer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
