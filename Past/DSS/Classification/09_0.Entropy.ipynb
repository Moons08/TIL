{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 엔트로피\n",
    "\n",
    "1. 확률변수 Y가 이산 확률변수\n",
    "1. K는 X가 가질 수 있는 클래스의 수\n",
    "1. $P(y)$는 확률 질량 함수\n",
    "\n",
    "$$H[Y] = -\\sum_{k=1}^K p(y_k) \\log_2 p(y_k)$$\n",
    "\n",
    "Y가 연속일 경우\n",
    "\n",
    "$$H[Y] = -\\int p(y) \\log_2 p(y) \\; dy$$\n",
    "\n",
    "\n",
    "$p(y) = 0 $인 경우 다음과 같은 극한값 사용\n",
    "- 로피탈 정리\n",
    "$$\\lim_{p\\rightarrow 0} \\; p\\log_2{p} = 0$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 엔트로피의 성질\n",
    "만약 확률분포가 결정론적이면(즉, 특정 하나의 값이 나올 확률이 1이라면) 엔프로피는 최솟값인 0\n",
    "\n",
    "엔트로피의 최댓값은 이산 확률변수의 클래스의 갯수에 따라 달라짐.\n",
    "- 클래스가 $2^K$개이고 각 클래스가 모두 같은 확률을 가질 경우\n",
    "\n",
    "$$H = -\\frac{2^K}{2^K}\\log_2\\dfrac{1}{2^K} = K$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 엔트로피와 정보량\n",
    "\n",
    "엔트로피 \n",
    "\n",
    "= 확률변수가 담을 수 있는 정보량\n",
    "\n",
    "= 확률변수의 표본값을 관측해서 얻을 수 있는 정보의 종류\n",
    "\n",
    "1. 엔트로피가 0 이면,\n",
    "1. 확률 변수는 결정론적 \n",
    "1. 확률 변수의 표본값이 변화하지 않음\n",
    "1. 따라서 표본값을 관측한다고 해도 추가 정보를 얻을 수 없음\n",
    "\n",
    "\n",
    ">1. 엔트로피가 크다면,\n",
    "1. 확률 변수의 표본값이 가질 수 있는 실질 경우의 수가 증가\n",
    "1. 표본값이 가진 정보량이 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 표본 데이터가 주어진 경우\n",
    "\n",
    "확률 변수 모형, 즉 이론적인 확률 밀도(질량) 함수가 아닌 실제 데이터가 주어진 경우에는 확률질량함수를 추정하여 엔트로피를 계산한다.\n",
    "\n",
    "예를 들어 데이터가 모두 80개가 있고 그 중 Y = 0 인 데이터가 40개, Y = 1인 데이터가 40개 있는 경우는 엔트로피가 1이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- 1 / 2 * np.log2(1 / 2) - 1 / 2 * np.log2(1 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 데이터가 모두 60개가 있고 그 중 Y= 0 인 데이터가 20개, Y = 1인 데이터가 40개 있는 경우는 엔트로피가 약 0.92이다.\n",
    "\n",
    "$$P(y=0) = \\dfrac{20}{60} = \\dfrac{1}{3}$$\n",
    "\n",
    "$$P(y=1) = \\dfrac{40}{60} = \\dfrac{2}{3}$$\n",
    "\n",
    "$$H[Y] = -\\dfrac{1}{3}\\log_2\\left(\\dfrac{1}{3}\\right) -\\dfrac{2}{3}\\log_2\\left(\\dfrac{2}{3}\\right) = 0.92$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 / 3 * np.log2(1 / 3) - 2 / 3 * np.log2(2 / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 조건부 엔트로피\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "H[Y \\mid X] \n",
    "&=& - \\sum_i \\sum_j \\,p(x_i, y_j) \\log_2 p(y_j \\mid x_i) \\\\\n",
    "&=& - \\sum_i \\sum_j p(y_j \\mid x_i) p(x_i) \\log_2 p(y_j \\mid x_i) \\\\\n",
    "&=& - \\sum_i p(x_i) \\sum_j p(y_j \\mid x_i)  \\log_2 p(y_j \\mid x_i) \\\\\n",
    "&=& \\sum_i p(x_i) H[Y \\mid x_i] \\\\\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "조건부 엔트로피는  X 의 값에 의해 만들어지는 새로운  Y  확률분포의 가중평균임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크로스 엔트로피\n",
    "확률분포 $p(y), q(y)$의 크로스 엔트로피\n",
    "\n",
    "$$-\\sum_{k=1}^K p(y_k) \\log_2 q(y_k)$$\n",
    "\n",
    "주로 분류 문제의 목표값 분포와 예측값 분포를 **비교**하는데 사용.\n",
    "\n",
    "ex) 이진 분류\n",
    "$$- y \\log_2 \\theta - (1 - y) \\log_2 (1 - \\theta)$$\n",
    "\n",
    "이 값은 예측값이 정답과 같다면 0이고 다르면 다를수록 증가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쿨백-라이블러 발산\n",
    "Kullback-Leibler divergence은 두 확률분포  $p(y) ,  q(y)$ 의 차이를 정량화하는 방법의 하나\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "KL(p(y) || q(y)) \n",
    "&=& -\\int p(y) \\log_2 q(y) \\, dy - \\left( -\\int p(y) \\log_2 p(y) \\, dy \\right) \\\\\n",
    "&=& \\int p(y) \\log_2 \\left(\\dfrac{p(y)}{q(y)}\\right) dy\\\\\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "크로스 엔트로피에서 대상이 되는 분포의 엔트로피는 뺀 값이므로 상대 엔트로피(relative entropy)라고도 한다.  \n",
    "1. 값은 항상 양수이며 \n",
    "1. 두 확률분포  p(x) ,  q(x) 가 완전히 같을 경우에만 0이 된다.\n",
    "1. p와 q의 순서가 바뀌면 값이 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지니 불순도\n",
    "Gini impurity.\n",
    "\n",
    "엔트로피와 유사한 개념. 지니 불순도는 엔트로피처럼 확률 분포가 어느쪽에 치우쳐있는가를 재는 척도지만 로그를 사용하지 않으므로 계산량이 더 적어 엔트로피 대신으로 많이 사용된다.\n",
    "\n",
    "$$G[Y] = -\\sum_{k=1}^K p(y_k) (1 - p(y_k))$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
