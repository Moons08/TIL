{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW\n",
    "bag of words.\n",
    "\n",
    "1. 전체 문서를 구성하는 고정된 단어장을 만들고 \n",
    "1. 개별 문서에 단어장에 해당하는 단어들이 포함되어 있는지 비교 후\n",
    "1. 횟수 or 있는지 없는지 체크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `DictVectorizer`:\n",
    "단어의 수를 세어놓은 사전에서 BOW 벡터를 만든다.\n",
    "- `CountVectorizer`:\n",
    "문서 집합으로부터 단어의 수를 세어 BOW 벡터를 만든다.\n",
    "- `TfidfVectorizer`:\n",
    "문서 집합으로부터 단어의 수를 세고 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 벡터를 만든다.\n",
    "- `HashingVectorizer`:\n",
    "hashing trick 을 사용하여 빠르게 BOW 벡터를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DictVectorizer\n",
    "사전 형태로 되어있는 feature 정보를 matrix형태로 변환\n",
    "- corpus 상의 각 단어의 사용 빈도를 나타내는 경우가 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0.],\n",
       "       [0., 3., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A': 1, 'B': 2}, {'B': 3, 'C': 1}]\n",
    "X = v.fit_transform(D) # fit / transform\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C']\n",
      "[{'A': 1.0, 'B': 2.0}, {'B': 3.0, 'C': 1.0}]\n",
      "[[0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "print(v.feature_names_)\n",
    "print(v.inverse_transform(X))\n",
    "print(v.transform({'C': 4, 'D': 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "`= tokenizing + counting + BOW`\n",
    "\n",
    "#### 다양한 인수를 가진다. 그 중 중요한 것들은 다음과 같다.\n",
    "\n",
    "- `stop_words` : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "- `analyzer` : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "- `tokenizer` : 함수 또는 None (디폴트)\n",
    "토큰 생성 함수 .\n",
    "- `token_pattern` : string\n",
    "토큰 정의용 정규 표현식\n",
    "- `ngram_range` : (min_n, max_n) 튜플\n",
    "n-그램 범위\n",
    "- `max_df` : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최대 빈도\n",
    "- `min_df` : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "단어장에 포함되기 위한 최소 빈도\n",
    "- `vocabulary` : 사전이나 리스트\n",
    "단어장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'last': 4,\n",
       " 'one': 5,\n",
       " 'second': 6,\n",
       " 'the': 7,\n",
       " 'third': 8,\n",
       " 'this': 9}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',    \n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_ # 각 단어에 이름표(숫자)를 붙임 - 단어장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['This is the second document.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "단어장을 생성할 때 무시할 수 있는 단어를 넣는다. (필수)\n",
    "- 너무 많은 단어는 오히려 판별에 방해가 되기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'last': 2, 'one': 3, 'second': 4, 'third': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\").fit(corpus) # 기본세팅 stop words\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token\n",
    "analyzer, tokenizer, token_pattern 등의 인수로 토큰 생성기 선택 가능\n",
    "- analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
    "\n",
    "\n",
    "- tokenizer : Override the string tokenization step while preserving the preprocessing and n-grams generation steps.\n",
    "\n",
    "- token_pattern : customize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " '?': 2,\n",
       " 'a': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'l': 10,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " 'o': 13,\n",
       " 'r': 14,\n",
       " 's': 15,\n",
       " 't': 16,\n",
       " 'u': 17}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'third': 1, 'this': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '?': 1,\n",
       " 'and': 2,\n",
       " 'document': 3,\n",
       " 'first': 4,\n",
       " 'is': 5,\n",
       " 'last': 6,\n",
       " 'one': 7,\n",
       " 'second': 8,\n",
       " 'the': 9,\n",
       " 'third': 10,\n",
       " 'this': 11}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "vect = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram\n",
    "단어장 생성에 사용할 토큰의 크기 결정(chunk 크기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and the': 0,\n",
       " 'first document': 1,\n",
       " 'is the': 2,\n",
       " 'is this': 3,\n",
       " 'last document': 4,\n",
       " 'second document': 5,\n",
       " 'second second': 6,\n",
       " 'the first': 7,\n",
       " 'the last': 8,\n",
       " 'the second': 9,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'this is': 12,\n",
       " 'this the': 13}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_ # 2개로만 묶음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0, 'the third': 1, 'third': 2, 'this': 3, 'this the': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2), token_pattern=\"t\\w+\").fit(corpus)\n",
    "vect.vocabulary_ # 1개 + 2개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수 \n",
    "`max_df`, `min_df` 인수 사용으로 문서에서 토큰이 나타난 횟수를 기준으로 구성가능\n",
    "- 인수 값이 int : 횟수\n",
    "- 인수 값이 float : 비중\n",
    "\n",
    "너무 적은 단어는 overfitting 발생할 수 있기 때문, 많은 단어는 의미가 없기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'document': 0, 'first': 1, 'is': 2, 'this': 3},\n",
       " {'and', 'last', 'one', 'second', 'the', 'third'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "vect.vocabulary_, vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Tern Frequency - Inverse Document Frequency \n",
    "- 문서 안에서 단어가 나오는 빈도 - 문서에 단어가 나올 빈도\n",
    "- 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 가중치를 축소하는 방법 (why? 문서 구별 능력이 떨어진다고 판단)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick\n",
    "- `CountVectorizer`는 모든 작업을 메모리 상에서 수행하므로 처리할 문서의 크기가 커지면 속도가 느려지거나 실행이 불가능해진다. \n",
    "- `HashingVectorizer`를 사용하면 해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty = fetch_20newsgroups()\n",
    "len(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 55.1 ms, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time CountVectorizer().fit(twenty.data).transform(twenty.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 s, sys: 39.7 ms, total: 4.13 s\n",
      "Wall time: 4.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112863 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "%time hv.transform(twenty.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import string\n",
    "from konlpy.utils import pprint\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "f = urlopen(\"https://www.datascienceschool.net/download-notebook/708e711429a646818b9dcbb581e0c10a/\")\n",
    "json = json.loads(f.read())\n",
    "cell = [\"\\n\".join(c[\"source\"]) for c in json[\"cells\"] if c[\"cell_type\"] == \"markdown\"]\n",
    "docs = [w for w in hannanum.nouns(\" \".join(cell)) if ((not w[0].isnumeric()) and (w[0] not in string.punctuation))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0482e2da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "vect = CountVectorizer().fit(docs)\n",
    "count = vect.transform(docs).toarray().sum(axis=0)\n",
    "idx = np.argsort(-count)\n",
    "count = count[idx]\n",
    "feature_name = np.array(vect.get_feature_names())[idx]\n",
    "plt.bar(range(len(count)), count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('컨테이너', 81),\n",
      " ('도커', 41),\n",
      " ('명령', 34),\n",
      " ('이미지', 33),\n",
      " ('사용', 26),\n",
      " ('가동', 14),\n",
      " ('중지', 13),\n",
      " ('mingw64', 13),\n",
      " ('삭제', 12),\n",
      " ('이름', 11),\n",
      " ('아이디', 11),\n",
      " ('다음', 10),\n",
      " ('시작', 9),\n",
      " ('목록', 8),\n",
      " ('옵션', 6),\n",
      " ('a181562ac4d8', 6),\n",
      " ('입력', 6),\n",
      " ('외부', 5),\n",
      " ('출력', 5),\n",
      " ('해당', 5),\n",
      " ('호스트', 5),\n",
      " ('명령어', 5),\n",
      " ('확인', 5),\n",
      " ('경우', 5),\n",
      " ('재시작', 4),\n",
      " ('존재', 4),\n",
      " ('컴퓨터', 4),\n",
      " ('터미널', 4),\n",
      " ('프롬프트', 4),\n",
      " ('포트', 4),\n",
      " ('377ad03459bf', 3),\n",
      " ('가상', 3),\n",
      " ('수행', 3),\n",
      " ('문자열', 3),\n",
      " ('dockeruser', 3),\n",
      " ('항목', 3),\n",
      " ('마찬가지', 3),\n",
      " ('대화형', 3),\n",
      " ('종료', 2),\n",
      " ('상태', 2),\n",
      " ('저장', 2),\n",
      " ('호스트간', 2),\n",
      " ('작업', 2),\n",
      " ('지정', 2),\n",
      " ('생각', 2),\n",
      " ('문헌', 2),\n",
      " ('동작', 2),\n",
      " ('시스템', 2),\n",
      " ('명시해', 2),\n",
      " ('특정', 2),\n",
      " ('관련하', 2),\n",
      " ('이때', 2),\n",
      " ('의미', 2),\n",
      " ('추가', 2),\n",
      " ('조합', 1),\n",
      " ('container', 1),\n",
      " ('폴더', 1),\n",
      " ('a1e4ed2ac65b', 1),\n",
      " ('작동', 1),\n",
      " ('자체', 1),\n",
      " ('자동', 1),\n",
      " ('image', 1),\n",
      " ('정지', 1),\n",
      " ('핵심', 1),\n",
      " ('초간단', 1),\n",
      " ('중복', 1),\n",
      " ('id', 1),\n",
      " ('최소한', 1),\n",
      " ('일부분', 1),\n",
      " ('컨테이', 1),\n",
      " ('daemon', 1),\n",
      " ('컨테이너상', 1),\n",
      " ('한다', 1),\n",
      " ('콜론', 1),\n",
      " ('태그', 1),\n",
      " ('하나', 1),\n",
      " ('툴박스', 1),\n",
      " ('파일', 1),\n",
      " ('포워딩', 1),\n",
      " ('주의해', 1),\n",
      " ('이해', 1),\n",
      " ('누른다', 1),\n",
      " ('이미지는', 1),\n",
      " ('공유', 1),\n",
      " ('브라우저', 1),\n",
      " ('복사', 1),\n",
      " ('문제', 1),\n",
      " ('문자', 1),\n",
      " ('관련', 1),\n",
      " ('명시', 1),\n",
      " ('길벗', 1),\n",
      " ('사용법', 1),\n",
      " ('메시지', 1),\n",
      " ('마지막', 1),\n",
      " ('리눅스', 1),\n",
      " ('나오기', 1),\n",
      " ('도서출판', 1),\n",
      " ('데몬', 1),\n",
      " ('대화적', 1),\n",
      " ('대표적', 1),\n",
      " ('내부', 1),\n",
      " ('머신', 1),\n",
      " ('이재홍', 1),\n",
      " ('사용자', 1),\n",
      " ('생략', 1),\n",
      " ('tag', 1),\n",
      " ('가능', 1),\n",
      " ('의존', 1),\n",
      " ('으로', 1),\n",
      " ('내용', 1),\n",
      " ('원본', 1),\n",
      " ('요약', 1),\n",
      " ('가지', 1),\n",
      " ('사용해', 1),\n",
      " ('오류', 1),\n",
      " ('연결', 1),\n",
      " ('여기', 1),\n",
      " ('개념', 1),\n",
      " ('실행', 1),\n",
      " ('시스템상', 1),\n",
      " ('소개', 1),\n",
      " ('설명', 1),\n",
      " ('생성', 1),\n",
      " ('연습', 1),\n",
      " ('윈도우즈', 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(feature_name, count)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
